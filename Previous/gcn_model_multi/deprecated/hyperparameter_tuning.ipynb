{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/virtual_envs/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.nn.models import GCN\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import optuna\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from reserve import generate_reserve\n",
    "from MDP_helpers import MDP, relabel_k\n",
    "from kmdp_toolbox import aStarAbs, sk_to_s\n",
    "from experiment import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 100 MDPs with 243 states and 5 actions \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:22<00:00,  4.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate Reserve Data\n",
    "N_datasets = 100\n",
    "\n",
    "N_sites = 5\n",
    "N_species = 20\n",
    "K = 7\n",
    "\n",
    "N_states = 3**N_sites\n",
    "N_actions = N_sites\n",
    "\n",
    "print(f\"Generating {N_datasets} MDPs with {N_states} states and {N_actions} actions \\n\")\n",
    "\n",
    "mdp_datasets = []\n",
    "for i in tqdm(range(N_datasets)):\n",
    "    pj = np.random.random() # random probability between 0 and 1\n",
    "\n",
    "    T, R = generate_reserve(N_sites, N_species, pj=pj, seed=i) \n",
    "    mdp = MDP(T, R, gamma=0.99)\n",
    "    mdp.solve_MDP()\n",
    "\n",
    "    mdp.k_states, mdp.K = aStarAbs(P=mdp.transitions, R=mdp.rewards, V=mdp.optimal_values, policy=mdp.optimal_policy, K=K, precision=1e-6)\n",
    "\n",
    "    mdp_datasets.append(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for i in range(len(mdp_datasets)):\n",
    "    # Set up data as graphs with node features defined by rewards and transition probabilities. Approach is to make dataset tabular as set node features\n",
    "    P = mdp_datasets[i].transitions\n",
    "    T = np.empty((N_states, N_states*N_actions))\n",
    "    for i in range(27):\n",
    "        T[i, :] = P[:, i, :].reshape(1, -1)\n",
    "\n",
    "    x = np.concatenate([T, mdp_datasets[i].rewards], axis=1)\n",
    "\n",
    "    # Cound whether transition are non-zero for any action\n",
    "    p_sum = np.sum((P> 0), axis=0)\n",
    "    edges = [[i, j] for i, j in zip(*np.where(p_sum > 0))]\n",
    "\n",
    "    # Convert to torch\n",
    "    x = torch.tensor(x, dtype=torch.float).to(device)\n",
    "    edges = torch.tensor(edges, dtype=torch.int).T.to(device)\n",
    "\n",
    "    k = torch.tensor(mdp_datasets[i].k_states, dtype=torch.int64).to(device)\n",
    "\n",
    "    T = torch.tensor(mdp_datasets[i].transitions).to('cpu')\n",
    "    R = torch.tensor(mdp_datasets[i].rewards).to('cpu')\n",
    "    V = torch.tensor(mdp_datasets[i].optimal_values).to('cpu')\n",
    "\n",
    "    dataset.append(\n",
    "        Data(\n",
    "            x=x,\n",
    "            edges=edges,\n",
    "            k_labels=k,\n",
    "            T = T,\n",
    "            R = R,\n",
    "            V = V\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = int(len(dataset)*0.8)\n",
    "train_data_hparam = dataset[:data_split]\n",
    "val_data = dataset[data_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildKMDP(T: torch.tensor, R: torch.tensor, predicted_k_states: torch.tensor, K: int) -> torch.tensor:\n",
    "    \"\"\" Implement buildKMDP using inbuilt torch functions to keep everything on device \"\"\"\n",
    "    K2S = sk_to_s(predicted_k_states, K)\n",
    "    weights = (1/torch.bincount(predicted_k_states))[predicted_k_states]\n",
    "\n",
    "    RK = torch.empty(size=(K, N_actions), device=device, dtype=torch.float64)\n",
    "    # R = torch.tensor(mdp.rewards).to(device)\n",
    "\n",
    "    TK = torch.empty(size=(N_actions, K, K), device=device, dtype=torch.float64)\n",
    "    # T = torch.tensor(mdp.transitions).to(device)\n",
    "\n",
    "    for k in range(K):\n",
    "        RK[k] = (R.T*weights).T[predicted_k_states==k].sum(axis=0)\n",
    "        for kp in range(K):\n",
    "            TK[:, k, kp] = (T[:, :, predicted_k_states==kp].sum(axis=2) * weights)[:, predicted_k_states==k].sum(axis=1)\n",
    "    return TK, RK, K2S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value iteration\n",
    "\n",
    "def valueIteration(T: torch.tensor, R: torch.tensor, gamma = 0.99, epsilon=1e-4, N_iter=10000) -> torch.tensor:\n",
    "    \"\"\" Implement Value Iteration in the pytorch environment \"\"\"\n",
    "    N_states, N_actions = R.shape\n",
    "    V = torch.zeros(size=[N_states], device=device, dtype=torch.float64)\n",
    "    Q = torch.empty(size=[N_states, N_actions], device=device, dtype=torch.float64)\n",
    "    for i in range(N_iter):\n",
    "        for a in range(N_actions):\n",
    "            Q[:, a] = R[:, a].T + gamma*T[a, :, :]@V\n",
    "\n",
    "        V_new, policy = Q.max(axis=1)\n",
    "\n",
    "        if torch.all(torch.abs(V_new - V) < epsilon):\n",
    "            break\n",
    "        \n",
    "        if i == N_iter - 1:\n",
    "            raise Exception(\"Did not converge in time. Consider increasing the number of iterations.\")\n",
    "\n",
    "        V = V_new\n",
    "    return V_new, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valueFunction(T, R, policy, gamma=0.99, epsilon=1e-3, N_iter = 1e6):\n",
    "    \"\"\" Calculate the value function of an mdp given a policy \"\"\"\n",
    "    N_states, N_actions = R.shape\n",
    "    \n",
    "    V = torch.zeros(size=[N_states])\n",
    "    V_new = torch.zeros(size=[N_states])\n",
    "\n",
    "    count = 0\n",
    "    converged=False\n",
    "    while not converged:\n",
    "        for s in range(N_states):\n",
    "            V_new[s] = R[s, policy[s]] + gamma*(T[policy[s], s]*V).sum()\n",
    "\n",
    "        if torch.max(V_new - V) < epsilon:\n",
    "            converged = True\n",
    "\n",
    "        V = 1*V_new\n",
    "\n",
    "        count += 1\n",
    "        if count >= N_iter:\n",
    "            print(\"Did not converge\")\n",
    "            break\n",
    "        \n",
    "    return V_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gap(T, R, V, predicted_k_states, K):\n",
    "    # predicted_k_states = #F.softmax(prediction, dim=1).argmax(axis=1)\n",
    "\n",
    "    new_K = len(predicted_k_states.unique())\n",
    "    predicted_k_states = relabel_k(predicted_k_states, K) if new_K != K else predicted_k_states\n",
    "\n",
    "    PK, RK, K2S = buildKMDP(T, R, predicted_k_states, new_K)\n",
    "    _, kmdp_policy = valueIteration(PK, RK, gamma=0.85, N_iter=50000, epsilon=1e-1)\n",
    "\n",
    "    k_policy = torch.empty(size=[N_states], dtype=torch.int64)\n",
    "\n",
    "    for k in range(new_K):\n",
    "        k_policy[K2S[k]] = kmdp_policy[k]\n",
    "\n",
    "    V_K = valueFunction(T, R, k_policy)\n",
    "\n",
    "    gap = torch.max(torch.abs(V - V_K))\n",
    "    error = gap/max(V)\n",
    "\n",
    "    return gap, error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT written\n",
    "def multiclass_recall_score(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Calculate the multiclass recall score using PyTorch.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true (torch.Tensor): True labels (ground truth).\n",
    "    - y_pred (torch.Tensor): Predicted labels.\n",
    "    - average (str): Type of averaging to use for multiclass recall.\n",
    "        - 'macro' (default): Calculate recall for each class and then take the average.\n",
    "        - 'micro': Calculate recall globally by considering all instances.\n",
    "        - 'weighted': Calculate recall for each class and weight them by support.\n",
    "\n",
    "    Returns:\n",
    "    - recall (float): The multiclass recall score.\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred), \"Input arrays must have the same length\"\n",
    "\n",
    "    if average not in ('macro', 'micro', 'weighted'):\n",
    "        raise ValueError(\"Invalid 'average' parameter. Use 'macro', 'micro', or 'weighted'.\")\n",
    "\n",
    "    num_classes = len(torch.unique(y_true))\n",
    "    recall_per_class = []\n",
    "\n",
    "    for class_label in range(num_classes):\n",
    "        true_positive = torch.sum((y_true == class_label) & (y_pred == class_label)).item()\n",
    "        false_negative = torch.sum((y_true == class_label) & (y_pred != class_label)).item()\n",
    "        recall = true_positive / (true_positive + false_negative + 1e-10)  # Adding a small epsilon to avoid division by zero\n",
    "        recall_per_class.append(recall)\n",
    "\n",
    "    if average == 'macro':\n",
    "        return sum(recall_per_class) / num_classes\n",
    "    elif average == 'micro':\n",
    "        total_true_positives = torch.sum((y_true == y_pred) & (y_true == class_label)).item()\n",
    "        total_false_negatives = torch.sum((y_true != y_pred) & (y_true == class_label)).item()\n",
    "        return total_true_positives / (total_true_positives + total_false_negatives + 1e-10)\n",
    "    elif average == 'weighted':\n",
    "        class_counts = [torch.sum(y_true == class_label).item() for class_label in range(num_classes)]\n",
    "        total_samples = len(y_true)\n",
    "        weights = [count / total_samples for count in class_counts]\n",
    "        weighted_recall = sum([recall_per_class[i] * weights[i] for i in range(num_classes)])\n",
    "        return weighted_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-22 11:50:35,532] A new study created in memory with name: no-name-6961b467-0dd7-482b-bce2-1b826f41f44d\n",
      "/tmp/ipykernel_38089/1842571840.py:10: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3571.)\n",
      "  Q[:, a] = R[:, a].T + gamma*T[a, :, :]@V\n",
      "[I 2023-10-22 11:51:04,588] Trial 0 finished with value: 1.6177703342496863 and parameters: {'hidden_channels': 175, 'num_layers': 1, 'dropout': 0.07546602597475609, 'lr': 6.075250423832699, 'weight_decay': 0.011337288794139148, 'gamma': 0.4421177965512437}. Best is trial 0 with value: 1.6177703342496863.\n",
      "[I 2023-10-22 11:51:59,226] Trial 1 finished with value: 1.8114474021554703 and parameters: {'hidden_channels': 91, 'num_layers': 2, 'dropout': 0.013750205729632793, 'lr': 8.318145491910569, 'weight_decay': 0.07679016681359671, 'gamma': 0.47433582812604225}. Best is trial 0 with value: 1.6177703342496863.\n",
      "[I 2023-10-22 11:52:26,053] Trial 2 finished with value: 1.6639993998269866 and parameters: {'hidden_channels': 75, 'num_layers': 1, 'dropout': 0.06351695253144367, 'lr': 5.585470708068748, 'weight_decay': 0.06909317037914851, 'gamma': 0.5210976336672664}. Best is trial 0 with value: 1.6177703342496863.\n",
      "[I 2023-10-22 11:53:22,410] Trial 3 finished with value: 1.8239935221251882 and parameters: {'hidden_channels': 160, 'num_layers': 2, 'dropout': 0.018520533081998813, 'lr': 7.535834165465281, 'weight_decay': 0.09531832066045207, 'gamma': 0.9345968075868946}. Best is trial 0 with value: 1.6177703342496863.\n",
      "[I 2023-10-22 11:54:18,944] Trial 4 finished with value: 1.710866403915051 and parameters: {'hidden_channels': 41, 'num_layers': 2, 'dropout': 0.07770483118427421, 'lr': 6.523446563611214, 'weight_decay': 0.04566606195877954, 'gamma': 0.37754483401040706}. Best is trial 0 with value: 1.6177703342496863.\n",
      "[I 2023-10-22 11:55:34,074] Trial 5 finished with value: 1.8244194500853697 and parameters: {'hidden_channels': 180, 'num_layers': 3, 'dropout': 0.08886089644052722, 'lr': 4.993738405445063, 'weight_decay': 0.005145342501098238, 'gamma': 0.18603627050623317}. Best is trial 0 with value: 1.6177703342496863.\n",
      "[I 2023-10-22 11:56:58,373] Trial 6 finished with value: 1.8239935221251882 and parameters: {'hidden_channels': 147, 'num_layers': 3, 'dropout': 0.0484616833191514, 'lr': 3.4628045806990384, 'weight_decay': 0.09753127323660024, 'gamma': 0.5637431721305082}. Best is trial 0 with value: 1.6177703342496863.\n",
      "[I 2023-10-22 11:58:24,342] Trial 7 finished with value: 1.7447844890031154 and parameters: {'hidden_channels': 157, 'num_layers': 3, 'dropout': 0.053016485138184, 'lr': 8.164599670435583, 'weight_decay': 0.011551473324438052, 'gamma': 0.7987500161474967}. Best is trial 0 with value: 1.6177703342496863.\n",
      "[I 2023-10-22 11:58:49,122] Trial 8 finished with value: 1.6381551868165076 and parameters: {'hidden_channels': 133, 'num_layers': 1, 'dropout': 0.037913923258795196, 'lr': 4.438687859962912, 'weight_decay': 0.07581325539896482, 'gamma': 0.7203369511258501}. Best is trial 0 with value: 1.6177703342496863.\n",
      "[I 2023-10-22 12:00:12,097] Trial 9 finished with value: 1.670145511685724 and parameters: {'hidden_channels': 195, 'num_layers': 3, 'dropout': 0.022739440697638027, 'lr': 9.803861469979532, 'weight_decay': 0.06722622744704303, 'gamma': 0.8509539187866739}. Best is trial 0 with value: 1.6177703342496863.\n",
      "[I 2023-10-22 12:00:40,451] Trial 10 finished with value: 1.8032476444846253 and parameters: {'hidden_channels': 103, 'num_layers': 1, 'dropout': 0.09360111944052786, 'lr': 0.8572607252975102, 'weight_decay': 0.026568739229608177, 'gamma': 0.09554029464821301}. Best is trial 0 with value: 1.6177703342496863.\n",
      "[I 2023-10-22 12:01:06,208] Trial 11 finished with value: 1.2563613344197184 and parameters: {'hidden_channels': 129, 'num_layers': 1, 'dropout': 0.04016179264041438, 'lr': 3.872502687795826, 'weight_decay': 0.04350845413197871, 'gamma': 0.7216142630155653}. Best is trial 11 with value: 1.2563613344197184.\n",
      "[I 2023-10-22 12:01:34,655] Trial 12 finished with value: 1.7364595672987417 and parameters: {'hidden_channels': 118, 'num_layers': 1, 'dropout': 0.06349419064759451, 'lr': 3.115732089994287, 'weight_decay': 0.030364530399527457, 'gamma': 0.6655381793153615}. Best is trial 11 with value: 1.2563613344197184.\n",
      "[I 2023-10-22 12:02:03,577] Trial 13 finished with value: 0.3005344048681545 and parameters: {'hidden_channels': 176, 'num_layers': 1, 'dropout': 0.004100687104164412, 'lr': 2.698927286009522, 'weight_decay': 0.0014830234258634764, 'gamma': 0.9970183847892857}. Best is trial 13 with value: 0.3005344048681545.\n",
      "[I 2023-10-22 12:02:30,547] Trial 14 finished with value: 0.39455751964865404 and parameters: {'hidden_channels': 124, 'num_layers': 1, 'dropout': 0.00457293517444131, 'lr': 1.9267335376927885, 'weight_decay': 0.04276461904234387, 'gamma': 0.9842328909875855}. Best is trial 13 with value: 0.3005344048681545.\n",
      "[I 2023-10-22 12:03:28,007] Trial 15 finished with value: 1.8239935221251882 and parameters: {'hidden_channels': 66, 'num_layers': 2, 'dropout': 0.000972904173250376, 'lr': 1.8921161431096314, 'weight_decay': 0.0015782755227792666, 'gamma': 0.9852372665983861}. Best is trial 13 with value: 0.3005344048681545.\n",
      "[I 2023-10-22 12:03:56,701] Trial 16 finished with value: 0.3412647409684365 and parameters: {'hidden_channels': 198, 'num_layers': 1, 'dropout': 0.00022102192258721676, 'lr': 0.23310045660815581, 'weight_decay': 0.022892223567141146, 'gamma': 0.9946119107873534}. Best is trial 13 with value: 0.3005344048681545.\n",
      "[I 2023-10-22 12:04:51,828] Trial 17 finished with value: 1.672831108240228 and parameters: {'hidden_channels': 191, 'num_layers': 2, 'dropout': 0.010891448820875067, 'lr': 0.13847634339571568, 'weight_decay': 0.0206416888548754, 'gamma': 0.8746356480050937}. Best is trial 13 with value: 0.3005344048681545.\n",
      "[I 2023-10-22 12:05:19,596] Trial 18 finished with value: 1.6305845262712992 and parameters: {'hidden_channels': 171, 'num_layers': 1, 'dropout': 2.5185671525838893e-05, 'lr': 0.025114543187056332, 'weight_decay': 0.018019251839355512, 'gamma': 0.872304867865897}. Best is trial 13 with value: 0.3005344048681545.\n",
      "[I 2023-10-22 12:05:46,887] Trial 19 finished with value: 0.3357030959060108 and parameters: {'hidden_channels': 199, 'num_layers': 1, 'dropout': 0.02397498537348186, 'lr': 2.2372615335223838, 'weight_decay': 0.00115017674538448, 'gamma': 0.9695905696681554}. Best is trial 13 with value: 0.3005344048681545.\n",
      "[I 2023-10-22 12:06:42,706] Trial 20 finished with value: 1.8114702748165095 and parameters: {'hidden_channels': 141, 'num_layers': 2, 'dropout': 0.023503596683863547, 'lr': 2.7093443882657104, 'weight_decay': 0.0035261083420992725, 'gamma': 0.8100784538915483}. Best is trial 13 with value: 0.3005344048681545.\n",
      "[I 2023-10-22 12:07:10,020] Trial 21 finished with value: 0.3303143738849389 and parameters: {'hidden_channels': 200, 'num_layers': 1, 'dropout': 0.010137510606270342, 'lr': 1.4706170727345398, 'weight_decay': 0.012731050581538728, 'gamma': 0.9954477476464959}. Best is trial 13 with value: 0.3005344048681545.\n",
      "[I 2023-10-22 12:07:37,168] Trial 22 finished with value: 1.0067778209512328 and parameters: {'hidden_channels': 200, 'num_layers': 1, 'dropout': 0.026455487401827828, 'lr': 2.012364245463768, 'weight_decay': 0.011800354297111817, 'gamma': 0.9233954640301728}. Best is trial 13 with value: 0.3005344048681545.\n",
      "[I 2023-10-22 12:08:05,050] Trial 23 finished with value: 0.3005783070444096 and parameters: {'hidden_channels': 181, 'num_layers': 1, 'dropout': 0.011082793721621138, 'lr': 1.437395049698391, 'weight_decay': 0.001151677714748363, 'gamma': 0.993383715080397}. Best is trial 13 with value: 0.3005344048681545.\n",
      "[I 2023-10-22 12:08:31,921] Trial 24 finished with value: 1.062955062500773 and parameters: {'hidden_channels': 178, 'num_layers': 1, 'dropout': 0.01093156500594901, 'lr': 1.2984779639848674, 'weight_decay': 0.013479244955726569, 'gamma': 0.8889886368381836}. Best is trial 13 with value: 0.3005344048681545.\n",
      "[I 2023-10-22 12:08:56,937] Trial 25 finished with value: 1.2047479576205258 and parameters: {'hidden_channels': 160, 'num_layers': 1, 'dropout': 0.01115690097008477, 'lr': 1.2213522167290414, 'weight_decay': 0.030512109758122165, 'gamma': 0.7759393814533068}. Best is trial 13 with value: 0.3005344048681545.\n",
      "[I 2023-10-22 12:09:54,086] Trial 26 finished with value: 1.8486431969856731 and parameters: {'hidden_channels': 183, 'num_layers': 2, 'dropout': 0.030300833430311282, 'lr': 2.856698752198598, 'weight_decay': 0.008354359226239463, 'gamma': 0.9064255670933684}. Best is trial 13 with value: 0.3005344048681545.\n",
      "[I 2023-10-22 12:10:22,522] Trial 27 finished with value: 1.2918090247141092 and parameters: {'hidden_channels': 165, 'num_layers': 1, 'dropout': 0.01638555739615314, 'lr': 4.10413495886907, 'weight_decay': 0.0020914784648897403, 'gamma': 0.8233578975267544}. Best is trial 13 with value: 0.3005344048681545.\n",
      "[I 2023-10-22 12:10:48,710] Trial 28 finished with value: 0.342056865510561 and parameters: {'hidden_channels': 149, 'num_layers': 1, 'dropout': 0.007411774870925007, 'lr': 0.9051400359124218, 'weight_decay': 0.020629964134900817, 'gamma': 0.9966296790961712}. Best is trial 13 with value: 0.3005344048681545.\n",
      "[I 2023-10-22 12:11:15,093] Trial 29 finished with value: 0.9331950421883136 and parameters: {'hidden_channels': 184, 'num_layers': 1, 'dropout': 0.01650280983670783, 'lr': 2.5844418043127337, 'weight_decay': 0.013937128008104909, 'gamma': 0.9151577022480342}. Best is trial 13 with value: 0.3005344048681545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3005344048681545\n",
      "{'hidden_channels': 176, 'num_layers': 1, 'dropout': 0.004100687104164412, 'lr': 2.698927286009522, 'weight_decay': 0.0014830234258634764, 'gamma': 0.9970183847892857}\n"
     ]
    }
   ],
   "source": [
    "N_epochs = 500\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_channels = trial.suggest_int(\"hidden_channels\", 30, 200)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0, 0.1)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-2, 10)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-3, 1e-1)\n",
    "\n",
    "    gamma = trial.suggest_float(\"gamma\", 0, 1)\n",
    "\n",
    "    gcn_model = GCN(\n",
    "        in_channels=dataset[0].x.shape[1], \n",
    "        out_channels=K, \n",
    "        hidden_channels=hidden_channels, \n",
    "        num_layers=num_layers, \n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(gcn_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    lr_sheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma)\n",
    "\n",
    "    gcn_model.train()\n",
    "    for epoch in range(N_epochs):\n",
    "        optimizer.zero_grad()     \n",
    "        loss = 0\n",
    "        for data in train_data_hparam:\n",
    "            pred = gcn_model(x = data.x, edge_index=data.edges)\n",
    "            loss += loss_function(pred, data.k_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_sheduler.step()\n",
    "    \n",
    "    gcn_model.eval()\n",
    "    errors = []\n",
    "    recall = []\n",
    "    for data in val_data:\n",
    "        out = gcn_model(x = data.x, edge_index=data.edges)\n",
    "        pred = F.softmax(out, dim=1).argmax(axis=1).to('cpu')\n",
    "\n",
    "        _, error = calculate_gap(data.T, data.R, data.V, pred, K)\n",
    "        errors.append(error.to('cpu'))\n",
    "        recall.append(\n",
    "            recall_score(data.k_labels.to('cpu'), pred.to('cpu'), average=\"macro\")\n",
    "        )\n",
    "    \n",
    "    return np.mean(errors) + (1 - np.mean(recall)) # Minimise errors while maximising recall score\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "best_params = study.best_params\n",
    "\n",
    "print(study.best_value)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "trials = [i for i in map(lambda x: dict([(\"score\", x.values[0]),*(x.params).items()]), study.get_trials())]\n",
    "trials = pd.DataFrame(trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score\thidden_channels\tnum_layers\tdropout\tlr\tweight_decay\tweight_param\tgamma\n",
    "21\t0.403245\t67\t1\t0.051031\t4.809483\t0.045045\t8\t0.972926\n",
    "20\t0.403374\t63\t1\t0.003532\t4.839212\t0.040630\t8\t0.989183\n",
    "14\t1.023662\t19\t1\t0.295559\t5.025916\t0.053842\t7\t0.798589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>hidden_channels</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>dropout</th>\n",
       "      <th>lr</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>gamma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.300534</td>\n",
       "      <td>176</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004101</td>\n",
       "      <td>2.698927</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.997018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.300578</td>\n",
       "      <td>181</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>1.437395</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.993384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.330314</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010138</td>\n",
       "      <td>1.470617</td>\n",
       "      <td>0.012731</td>\n",
       "      <td>0.995448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.335703</td>\n",
       "      <td>199</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023975</td>\n",
       "      <td>2.237262</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.969591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.341265</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.233100</td>\n",
       "      <td>0.022892</td>\n",
       "      <td>0.994612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.342057</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007412</td>\n",
       "      <td>0.905140</td>\n",
       "      <td>0.020630</td>\n",
       "      <td>0.996630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.394558</td>\n",
       "      <td>124</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004573</td>\n",
       "      <td>1.926734</td>\n",
       "      <td>0.042765</td>\n",
       "      <td>0.984233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.933195</td>\n",
       "      <td>184</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016503</td>\n",
       "      <td>2.584442</td>\n",
       "      <td>0.013937</td>\n",
       "      <td>0.915158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.006778</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026455</td>\n",
       "      <td>2.012364</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.923395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.062955</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010932</td>\n",
       "      <td>1.298478</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.888989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.204748</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011157</td>\n",
       "      <td>1.221352</td>\n",
       "      <td>0.030512</td>\n",
       "      <td>0.775939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.256361</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>0.040162</td>\n",
       "      <td>3.872503</td>\n",
       "      <td>0.043508</td>\n",
       "      <td>0.721614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.291809</td>\n",
       "      <td>165</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016386</td>\n",
       "      <td>4.104135</td>\n",
       "      <td>0.002091</td>\n",
       "      <td>0.823358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.617770</td>\n",
       "      <td>175</td>\n",
       "      <td>1</td>\n",
       "      <td>0.075466</td>\n",
       "      <td>6.075250</td>\n",
       "      <td>0.011337</td>\n",
       "      <td>0.442118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.630585</td>\n",
       "      <td>171</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.025115</td>\n",
       "      <td>0.018019</td>\n",
       "      <td>0.872305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.638155</td>\n",
       "      <td>133</td>\n",
       "      <td>1</td>\n",
       "      <td>0.037914</td>\n",
       "      <td>4.438688</td>\n",
       "      <td>0.075813</td>\n",
       "      <td>0.720337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.663999</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>0.063517</td>\n",
       "      <td>5.585471</td>\n",
       "      <td>0.069093</td>\n",
       "      <td>0.521098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.670146</td>\n",
       "      <td>195</td>\n",
       "      <td>3</td>\n",
       "      <td>0.022739</td>\n",
       "      <td>9.803861</td>\n",
       "      <td>0.067226</td>\n",
       "      <td>0.850954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.672831</td>\n",
       "      <td>191</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010891</td>\n",
       "      <td>0.138476</td>\n",
       "      <td>0.020642</td>\n",
       "      <td>0.874636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.710866</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>0.077705</td>\n",
       "      <td>6.523447</td>\n",
       "      <td>0.045666</td>\n",
       "      <td>0.377545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.736460</td>\n",
       "      <td>118</td>\n",
       "      <td>1</td>\n",
       "      <td>0.063494</td>\n",
       "      <td>3.115732</td>\n",
       "      <td>0.030365</td>\n",
       "      <td>0.665538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.744784</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0.053016</td>\n",
       "      <td>8.164600</td>\n",
       "      <td>0.011551</td>\n",
       "      <td>0.798750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.803248</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>0.093601</td>\n",
       "      <td>0.857261</td>\n",
       "      <td>0.026569</td>\n",
       "      <td>0.095540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.811447</td>\n",
       "      <td>91</td>\n",
       "      <td>2</td>\n",
       "      <td>0.013750</td>\n",
       "      <td>8.318145</td>\n",
       "      <td>0.076790</td>\n",
       "      <td>0.474336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.811470</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "      <td>0.023504</td>\n",
       "      <td>2.709344</td>\n",
       "      <td>0.003526</td>\n",
       "      <td>0.810078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.823994</td>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>1.892116</td>\n",
       "      <td>0.001578</td>\n",
       "      <td>0.985237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.823994</td>\n",
       "      <td>160</td>\n",
       "      <td>2</td>\n",
       "      <td>0.018521</td>\n",
       "      <td>7.535834</td>\n",
       "      <td>0.095318</td>\n",
       "      <td>0.934597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.823994</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "      <td>0.048462</td>\n",
       "      <td>3.462805</td>\n",
       "      <td>0.097531</td>\n",
       "      <td>0.563743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.824419</td>\n",
       "      <td>180</td>\n",
       "      <td>3</td>\n",
       "      <td>0.088861</td>\n",
       "      <td>4.993738</td>\n",
       "      <td>0.005145</td>\n",
       "      <td>0.186036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.848643</td>\n",
       "      <td>183</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030301</td>\n",
       "      <td>2.856699</td>\n",
       "      <td>0.008354</td>\n",
       "      <td>0.906426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       score  hidden_channels  num_layers   dropout        lr  weight_decay  \\\n",
       "13  0.300534              176           1  0.004101  2.698927      0.001483   \n",
       "23  0.300578              181           1  0.011083  1.437395      0.001152   \n",
       "21  0.330314              200           1  0.010138  1.470617      0.012731   \n",
       "19  0.335703              199           1  0.023975  2.237262      0.001150   \n",
       "16  0.341265              198           1  0.000221  0.233100      0.022892   \n",
       "28  0.342057              149           1  0.007412  0.905140      0.020630   \n",
       "14  0.394558              124           1  0.004573  1.926734      0.042765   \n",
       "29  0.933195              184           1  0.016503  2.584442      0.013937   \n",
       "22  1.006778              200           1  0.026455  2.012364      0.011800   \n",
       "24  1.062955              178           1  0.010932  1.298478      0.013479   \n",
       "25  1.204748              160           1  0.011157  1.221352      0.030512   \n",
       "11  1.256361              129           1  0.040162  3.872503      0.043508   \n",
       "27  1.291809              165           1  0.016386  4.104135      0.002091   \n",
       "0   1.617770              175           1  0.075466  6.075250      0.011337   \n",
       "18  1.630585              171           1  0.000025  0.025115      0.018019   \n",
       "8   1.638155              133           1  0.037914  4.438688      0.075813   \n",
       "2   1.663999               75           1  0.063517  5.585471      0.069093   \n",
       "9   1.670146              195           3  0.022739  9.803861      0.067226   \n",
       "17  1.672831              191           2  0.010891  0.138476      0.020642   \n",
       "4   1.710866               41           2  0.077705  6.523447      0.045666   \n",
       "12  1.736460              118           1  0.063494  3.115732      0.030365   \n",
       "7   1.744784              157           3  0.053016  8.164600      0.011551   \n",
       "10  1.803248              103           1  0.093601  0.857261      0.026569   \n",
       "1   1.811447               91           2  0.013750  8.318145      0.076790   \n",
       "20  1.811470              141           2  0.023504  2.709344      0.003526   \n",
       "15  1.823994               66           2  0.000973  1.892116      0.001578   \n",
       "3   1.823994              160           2  0.018521  7.535834      0.095318   \n",
       "6   1.823994              147           3  0.048462  3.462805      0.097531   \n",
       "5   1.824419              180           3  0.088861  4.993738      0.005145   \n",
       "26  1.848643              183           2  0.030301  2.856699      0.008354   \n",
       "\n",
       "       gamma  \n",
       "13  0.997018  \n",
       "23  0.993384  \n",
       "21  0.995448  \n",
       "19  0.969591  \n",
       "16  0.994612  \n",
       "28  0.996630  \n",
       "14  0.984233  \n",
       "29  0.915158  \n",
       "22  0.923395  \n",
       "24  0.888989  \n",
       "25  0.775939  \n",
       "11  0.721614  \n",
       "27  0.823358  \n",
       "0   0.442118  \n",
       "18  0.872305  \n",
       "8   0.720337  \n",
       "2   0.521098  \n",
       "9   0.850954  \n",
       "17  0.874636  \n",
       "4   0.377545  \n",
       "12  0.665538  \n",
       "7   0.798750  \n",
       "10  0.095540  \n",
       "1   0.474336  \n",
       "20  0.810078  \n",
       "15  0.985237  \n",
       "3   0.934597  \n",
       "6   0.563743  \n",
       "5   0.186036  \n",
       "26  0.906426  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.sort_values(by=\"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(savefile=\"hparams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trials.index:\n",
    "    trials.loc[i].to_dict()\n",
    "    experiment.save(trials.loc[i].to_dict())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f50ed3b5ee24e8ebac96f2a320b6a529fb0cbd6f6197cf3a5968f4ecab23d005"
  },
  "kernelspec": {
   "display_name": "Python 3.10.12 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
